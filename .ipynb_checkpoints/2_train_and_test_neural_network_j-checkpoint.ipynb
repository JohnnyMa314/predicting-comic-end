{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 短命作品の学習と予測\n",
    "\n",
    "　目次情報を学習して，短命作品を予測してみます．\n",
    "\n",
    "## 環境構築\n",
    "\n",
    "```bash\n",
    "conda env create -f env.yml\n",
    "```\n",
    "\n",
    "## 問題設定\n",
    "\n",
    "　本記事の目的は，2週目から7週目までの掲載順を入力とし，当該作品が短命作品（10週以内に終了する作品）か否かを予測することです．1週目の掲載順を除外したのは，新連載作品はほとんど巻頭に掲載されるためです．また，近年最短とされる作品が8週連載なので，遅くともこの1週間前には打ち切りを予測するため，7週目までの掲載順を入力としました．打ち切りを阻止するためには，もっと早期に打ち切りを予測する必要がありますが，これは今後の課題ということで…．\n",
    "\n",
    "## 目次データ\n",
    "\n",
    "　[0_obtain_comic_data_j.ipynb](0_obtain_comic_data_j.ipynb)で取得した`data/wj-api.json`を使います．また，[1_analyze_comic_data_j.ipynb](1_analyze_comic_data_j.ipynb)で定義した`ComicAnalyzer`を`analyze.py`からimportして使います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import analize\n",
    "\n",
    "wj = analize.ComicAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル\n",
    "\n",
    "　以下に，本記事で想定する[多層パーセプトロン](https://en.wikipedia.org/wiki/Multilayer_perceptron)のモデルを示します．多層パーセプトロンについては，[誤差逆伝播法のノート](http://qiita.com/Ugo-Nama/items/04814a13c9ea84978a4c)が詳しいです．\n",
    "\n",
    "![model.png](fig/model.png)\n",
    "\n",
    "　本記事では，隠れ層1層の多層パーセプトロンを用います．入力層は6ノード（2週目から7週目までの掲載順），隠れ層は$n$ノード，出力層は1ノード（短命作品である確率）を含みます．隠れ層および出力層の活性化関数として，[Sigmoid関数](https://ja.wikipedia.org/wiki/%E6%B4%BB%E6%80%A7%E5%8C%96%E9%96%A2%E6%95%B0#.E3.82.B7.E3.82.B0.E3.83.A2.E3.82.A4.E3.83.89.E9.96.A2.E6.95.B0)を用います．学習率として，$r$を用います．最適化アルゴリズムとして，オーソドックスな[Stochastic Gradient Descent (SGD)](https://ja.wikipedia.org/wiki/%E7%A2%BA%E7%8E%87%E7%9A%84%E5%8B%BE%E9%85%8D%E9%99%8D%E4%B8%8B%E6%B3%95#cite_note-17)と，人気の[Adam](https://arxiv.org/abs/1412.6980)の両方を使って結果を比較します．\n",
    " \n",
    "　本記事では，以下のハイパーパラメータを調整して，予測性能の最大化を目指します．\n",
    "\n",
    "|項目|設計空間|補足|\n",
    "|:--|:--|:--|\n",
    "|学習率（$r$）| $0 < r < 1$| 大きいほど収束が速いが，大きすぎるとうまく学習しない．|\n",
    "|隠れ層のノード数（$n$）| $\\{1, 2,...,8\\}$ |最大でも入力層＋出力層が目安？|\n",
    "|最適化アルゴリズム| {Gradient descent, Adam}| 前者はオーソドックス，Adamは新しめ．|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 学習データ，テストデータの生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def make_x(anlzr, title='ONE PIECE'):\n",
    "    worsts = np.array(anlzr.extract_item(title)[1:7])\n",
    "    bests = np.array(anlzr.extract_item(title, 'best')[1:7])\n",
    "    worsts_normalized = worsts / (worsts + bests - 1)    \n",
    "    return worsts_normalized\n",
    "\n",
    "\n",
    "def make_y(anlzr, title='ONE PIECE', thresh_week=10):\n",
    "    return float(len(anlzr.extract_item(title)) <=  thresh_week)\n",
    "\n",
    "\n",
    "def batch_x_y(anlzr, titles, thresh_week=10):\n",
    "    xs = np.array([make_x(anlzr, title) for title in titles])\n",
    "    ys = np.array([[make_y(anlzr, title, thresh_week)] for title in titles])\n",
    "    return xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_x, test_y = batch_x_y(wj, wj.end_titles[-100:])\n",
    "train_x, train_y = batch_x_y(wj, wj.end_titles[:-100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多層パーセプトロンの構築\n",
    "\n",
    "　`ComicNet()`は，多層パーセプトロンを管理するクラスです．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class ComicNet():\n",
    "    def __init__(self, train_x, train_y, test_x, test_y,\n",
    "                 r=0.01, n=7, epoch=1000, algo='GD'):\n",
    "        self.train_x = train_x\n",
    "        self.train_y = train_y\n",
    "        self.test_x = test_x\n",
    "        self.test_y = test_y\n",
    "        self.learning_rate = r\n",
    "        self.n_hidden = n\n",
    "        self.epoch = epoch\n",
    "        self.algo = algo\n",
    "                \n",
    "    def build_network(self):\n",
    "        n_input = self.test_x.shape[1]\n",
    "        n_output = self.test_y.shape[1]\n",
    "        self.x = tf.placeholder(tf.float32, [None, n_input], name='x')\n",
    "        self.y = tf.placeholder(tf.float32, [None, n_output], name='y')\n",
    "        \n",
    "        self.w_hidden = tf.Variable(\n",
    "            tf.truncated_normal((n_input, self.n_hidden), stddev=0.05))\n",
    "        self.b_hidden = tf.Variable(tf.zeros(self.n_hidden))\n",
    "        self.logits = tf.add(tf.matmul(self.x, self.w_hidden), self.b_hidden)\n",
    "        self.logits = tf.nn.sigmoid(self.logits)\n",
    "        \n",
    "        self.w_output = tf.Variable(\n",
    "            tf.truncated_normal((self.n_hidden, n_output), stddev=0.05))\n",
    "        self.b_output = tf.Variable(tf.zeros(n_output))\n",
    "        self.logits = tf.add(tf.matmul(self.logits, self.w_output), self.b_output)\n",
    "        self.logits = tf.nn.sigmoid(self.logits)\n",
    "        \n",
    "        self.loss = tf.nn.l2_loss(self.y - self.logits, name='loss')\n",
    "        \n",
    "        if self.algo == 'GD':\n",
    "            self.optimizer = tf.train.GradientDescentOptimizer(\n",
    "                self.learning_rate).minimize(self.loss)\n",
    "        else:\n",
    "            self.optimizer = tf.train.AdamOptimizer(\n",
    "                self.learning_rate).minimize(self.loss)\n",
    "        \n",
    "        correct_prediction = tf.equal(self.y, tf.round(self.logits))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32),\n",
    "            name='accuracy')\n",
    "    \n",
    "    def batch_train_and_valid(self, size):\n",
    "        for start in range(0, len(self.train_x), size):\n",
    "            end = min(start + size, len(self.train_x))\n",
    "            tra_x = np.delete(self.train_x, range(start, end), 0)\n",
    "            tra_y = np.delete(self.train_y, range(start, end), 0)\n",
    "            val_x = self.train_x[start:end]\n",
    "            val_y = self.train_y[start:end]\n",
    "            yield tra_x, tra_y, val_x, val_y\n",
    "    \n",
    "    def train(self):\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            for e in range(self.epoch):\n",
    "                for tra_x, tra_y, val_x, val_y in self.batch_train_and_valid(128): \n",
    "                    feed_dict = {self.x: tra_x, self.y: tra_y}\n",
    "                    _, tra_loss, tra_acc = sess.run(\n",
    "                        (self.optimizer, self.loss, self.accuracy), \n",
    "                        feed_dict=feed_dict)\n",
    "                    feed_dict = {self.x: val_x, self.y: val_y}\n",
    "                    val_loss, val_acc = sess.run(\n",
    "                        (self.loss, self.accuracy), feed_dict=feed_dict)\n",
    "                if e%500 == 0:\n",
    "                    print('epoch: ', str(e), 'tra_loss: ', str(tra_loss),\n",
    "                          'val_acc: ', str(val_acc))\n",
    "            \n",
    "            self.save_model_path = './prediction_model'\n",
    "            saver = tf.train.Saver()\n",
    "            save_path = saver.save(sess, self.save_model_path)\n",
    "            \n",
    "            # for debug.\n",
    "            feed_dict = {self.x: train_x, self.y: train_y}\n",
    "            final_logits = sess.run(self.logits, feed_dict)\n",
    "            return final_logits\n",
    "\n",
    "            \n",
    "    def test(self):\n",
    "        loaded_graph = tf.Graph()\n",
    "        \n",
    "        with tf.Session(graph=loaded_graph) as sess:\n",
    "            loader = tf.train.import_meta_graph(\n",
    "                self.save_model_path + '.meta')\n",
    "            loader.restore(sess, self.save_model_path)\n",
    "            \n",
    "            loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "            loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "            loaded_loss = loaded_graph.get_tensor_by_name('loss:0')\n",
    "        \n",
    "            feed_dict = {loaded_x: self.test_x, loaded_y: self.test_y}\n",
    "            test_loss = sess.run(loaded_loss, feed_dict=feed_dict)\n",
    "            print('test_loss:', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "wjnet = ComicNet(train_x, train_y, test_x, test_y, epoch=1000, r=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "wjnet.build_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 tra_loss:  42.0165 val_acc:  0.935185\n",
      "epoch:  500 tra_loss:  26.7009 val_acc:  0.935185\n"
     ]
    }
   ],
   "source": [
    "logits = wjnet.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
