{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 短命作品の学習と予測\n",
    "\n",
    "　目次情報を学習して，短命作品を予測してみます．\n",
    "\n",
    "## 環境構築\n",
    "\n",
    "```bash\n",
    "conda env create -f env.yml\n",
    "```\n",
    "\n",
    "## 目次データ\n",
    "\n",
    "　[0_obtain_comic_data_j.ipynb](0_obtain_comic_data_j.ipynb)で取得した`data/wj-api.json`を使います．また，[1_analyze_comic_data_j.ipynb](1_analyze_comic_data_j.ipynb)で定義した`ComicAnalyzer`を`comic.py`からimportして使います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import comic\n",
    "\n",
    "wj = comic.ComicAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル\n",
    "\n",
    "　何週目までの掲載順を用いるのが適当かわからないので，本記事では，入力の異なる合計7種類の[多層パーセプトロン](https://en.wikipedia.org/wiki/Multilayer_perceptron)を実装します．多層パーセプトロンついては，[誤差逆伝播法のノート](http://qiita.com/Ugo-Nama/items/04814a13c9ea84978a4c)が詳しいです．\n",
    "\n",
    "![model1.png](fig/model1.png)\n",
    "![model2.png](fig/model2.png)\n",
    "![model7.png](fig/model7.png)\n",
    "\n",
    "　モデル1は1週目のみ，モデル2は2週目まで，…，モデル7は7週目までの掲載順を入力に用います．稀にですが，1週目に巻頭以外に掲載される作品がありますので，1週目の掲載順も入力に加えます．[CS231n Convolutional Neural Networks for Visual Recognition](https://cs231n.github.io/neural-networks-1/)を参考に，隠れ層は3層にします．簡単のため，各隠れ層のノード数は同じ，と仮定します．隠れ層の活性化関数として，[ReLU](https://ja.wikipedia.org/wiki/%E6%B4%BB%E6%80%A7%E5%8C%96%E9%96%A2%E6%95%B0#ReLU.EF.BC.88.E3.83.A9.E3.83.B3.E3.83.97.E9.96.A2.E6.95.B0.EF.BC.89)を使います．出力層は短命作品である確率を出力し，活性化関数として[Sigmoid](https://ja.wikipedia.org/wiki/%E6%B4%BB%E6%80%A7%E5%8C%96%E9%96%A2%E6%95%B0#.E3.82.B7.E3.82.B0.E3.83.A2.E3.82.A4.E3.83.89.E9.96.A2.E6.95.B0)を使います．学習には，学習率$r$の[Adam](https://arxiv.org/abs/1412.6980)を使います．隠れ層のノード数$n$と学習率$r$は，モデルごとに，[TensorBoard](https://www.tensorflow.org/get_started/summaries_and_tensorboard)で遊びながらチューニングします．\n",
    " \n",
    " 　真面目にやるなら，活性化関数や，変数の初期値分布や，最適化アルゴリズムもハイパーパラメータとしてチューニングするべきだとは思います．今回は簡単のため，隠れ層と学習率のみ調整します．それもかなりざっくりと．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実装\n",
    "\n",
    "　多層パーセプトロンを管理するためのクラス`ComicNet()`を以下に定義します．`ComicNet()`は，各種データ（学習，検証，およびテスト）の生成，多層パーセプトロンの構築，学習，およびテストを実行できます．実装には，[TensorFlow](https://www.tensorflow.org/)を用います．[TensorFlow](https://www.tensorflow.org/)については，[特にプログラマーでもデータサイエンティストでも\bないけど、Tensorflowを1ヶ月触ったので超分かりやすく解説](http://qiita.com/tawago/items/c977c79b76c5979874e8)が詳しいです．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class ComicNet():\n",
    "    \"\"\" マンガ作品が短命か否かを識別する多層パーセプトロンを管理するクラスです．\n",
    "    \n",
    "    - analyzer：ComicAnalyzerクラスのオブジェクト．\n",
    "    - thresh_week：短命作品とそれ以外を分けるしきい値．\n",
    "    - model：多層パーセプトロンに入力する掲載週の数．入力層のノード数．\n",
    "    - batch_size：学習データのバッチサイズ．検証データのサイズ．\n",
    "    \"\"\"\n",
    "    def __init__(self, analyzer, thresh_week=10, model=6, batch_size=128):\n",
    "        self.n_x = model\n",
    "        self.batch_size = batch_size\n",
    "        self.x_test, self.y_test = self.get_xs_ys(\n",
    "            analyzer, analyzer.end_titles[-100:],\n",
    "            thresh_week)\n",
    "        self.x_val, self.y_val = self.get_xs_ys(\n",
    "            analyzer, analyzer.end_titles[- 100 - batch_size:-100],\n",
    "            thresh_week)\n",
    "        self.x_tra, self.y_tra = self.get_xs_ys(\n",
    "            analyzer, analyzer.end_titles[:- 100 - batch_size],\n",
    "            thresh_week)\n",
    "    \n",
    "    \n",
    "    def get_x(self, analyzer, title):\n",
    "        \"\"\"指定された作品の指定週までの正規化掲載順を取得する関数です．\"\"\"\n",
    "        worsts = np.array(analyzer.extract_item(title)[:self.n_x])\n",
    "        bests = np.array(analyzer.extract_item(title, 'best')[:self.n_x])\n",
    "        bests_normalized = bests / (worsts + bests - 1)    \n",
    "        return bests_normalized\n",
    "\n",
    "    \n",
    "    def get_y(self, analyzer, title, thresh_week=10):\n",
    "        \"\"\"指定された作品が，短命作品か否かを取得する関数です．\"\"\"\n",
    "        return float(len(analyzer.extract_item(title)) <=  thresh_week)\n",
    "\n",
    "\n",
    "    def get_xs_ys(self, analyzer, titles, thresh_week=10):\n",
    "        \"\"\"指定された作品群の「正規化掲載順」と「短命作品か否か」を取得する関数です．\"\"\"\n",
    "        xs = np.array([self.get_x(analyzer, title) for title in titles])\n",
    "        ys = np.array([[self.get_y(analyzer, title, thresh_week)] \n",
    "                       for title in titles])\n",
    "        return xs, ys\n",
    "    \n",
    "    \n",
    "    def build_graph(self, r=0.01, n_h=4, stddev=0.05):\n",
    "        \"\"\"多層パーセプトロンを構築する関数です．\"\"\"\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # 入力層およびターゲット\n",
    "        n_y = self.y_test.shape[1]\n",
    "        self.x = tf.placeholder(tf.float32, [None, self.n_x], name='x')\n",
    "        self.y = tf.placeholder(tf.float32, [None, n_y], name='y')\n",
    "        \n",
    "        # 隠れ層\n",
    "        self.w_h_1 = tf.Variable(\n",
    "            tf.truncated_normal((self.n_x, n_h), stddev=stddev))\n",
    "        self.w_h_2 = tf.Variable(\n",
    "            tf.truncated_normal((n_h, n_h), stddev=stddev))\n",
    "        self.w_h_3 = tf.Variable(\n",
    "            tf.truncated_normal((n_h, n_h), stddev=stddev))\n",
    "        self.b_h_1 = tf.Variable(tf.zeros(n_h))\n",
    "        self.b_h_2 = tf.Variable(tf.zeros(n_h))\n",
    "        self.b_h_3 = tf.Variable(tf.zeros(n_h))\n",
    "        self.logits = tf.add(tf.matmul(self.x, self.w_h_1), self.b_h_1)\n",
    "        self.logits = tf.nn.relu(self.logits)\n",
    "        self.logits = tf.add(tf.matmul(self.logits, self.w_h_2), self.b_h_2)\n",
    "        self.logits = tf.nn.relu(self.logits)\n",
    "        self.logits = tf.add(tf.matmul(self.logits, self.w_h_3), self.b_h_3)\n",
    "        self.logits = tf.nn.relu(self.logits)\n",
    "        tf.summary.histogram('w_h_1', self.w_h_1)\n",
    "        tf.summary.histogram('w_h_2', self.w_h_2)\n",
    "        tf.summary.histogram('w_h_3', self.w_h_3)\n",
    "        tf.summary.histogram('b_h_1', self.b_h_1)\n",
    "        tf.summary.histogram('b_h_2', self.b_h_2)\n",
    "        tf.summary.histogram('b_h_3', self.b_h_3)\n",
    "        \n",
    "        # 出力層\n",
    "        self.w_y = tf.Variable(\n",
    "            tf.truncated_normal((n_h, n_y), stddev=stddev))\n",
    "        self.b_y = tf.Variable(tf.zeros(n_y))\n",
    "        self.logits = tf.add(tf.matmul(self.logits, self.w_y), self.b_y)\n",
    "        self.logits = tf.nn.sigmoid(self.logits, name='logits')\n",
    "        tf.summary.histogram('w_y', self.w_y)\n",
    "        tf.summary.histogram('b_y', self.b_y)\n",
    "        tf.summary.histogram('logits', self.logits)\n",
    "        \n",
    "        # コスト関数\n",
    "        self.loss = tf.nn.l2_loss(self.y - self.logits, name='loss')\n",
    "        tf.summary.scalar('loss', self.loss)\n",
    "        \n",
    "        # 最適化\n",
    "        self.optimizer = tf.train.AdamOptimizer(r).minimize(self.loss)\n",
    "        correct_prediction = tf.equal(self.y, tf.round(self.logits))\n",
    "        self.acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32),\n",
    "            name='acc')\n",
    "        tf.summary.scalar('acc', self.acc)\n",
    "        \n",
    "        self.merged = tf.summary.merge_all()\n",
    "            \n",
    "        \n",
    "    def train(self, print_loss=False, save_log=False, save_model=False,\n",
    "              epoch=10000, log_dir='./logs/1', log_name='', \n",
    "              model_path='./prediction_model'):\n",
    "        \"\"\"多層パーセプトロンを訓練し，ログや学習済みモデルを保存する関数です．\"\"\"\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer()) # 変数の初期化\n",
    "            \n",
    "            # ログ保存用の設定\n",
    "            log_tra = log_dir + '/train/' + log_name \n",
    "            writer_tra = tf.summary.FileWriter(log_tra)\n",
    "            log_val = log_dir + '/val/' + log_name\n",
    "            writer_val = tf.summary.FileWriter(log_val)        \n",
    "\n",
    "            for e in range(epoch):\n",
    "                # trainingバッチの生成\n",
    "                index_batch = np.random.choice(\n",
    "                    np.arange(len(self.x_tra)), self.batch_size, replace=False)\n",
    "                x_tra_batch = self.x_tra[index_batch]\n",
    "                y_tra_batch = self.y_tra[index_batch]\n",
    "                \n",
    "                # training\n",
    "                feed_dict = {self.x: x_tra_batch, self.y: y_tra_batch}\n",
    "                _, loss_tra, acc_tra, mer_tra = sess.run(\n",
    "                        (self.optimizer, self.loss, self.acc, self.merged), \n",
    "                        feed_dict=feed_dict)\n",
    "                \n",
    "                # validation\n",
    "                feed_dict = {self.x: self.x_val, self.y: self.y_val}\n",
    "                loss_val, acc_val, mer_val = sess.run(\n",
    "                    (self.loss, self.acc, self.merged),\n",
    "                    feed_dict=feed_dict)\n",
    "                \n",
    "                # ログの保存\n",
    "                if save_log:\n",
    "                    writer_tra.add_summary(mer_tra, e)\n",
    "                    writer_val.add_summary(mer_val, e)\n",
    "                \n",
    "                # コスト関数の出力\n",
    "                if print_loss and e % 500 == 0:\n",
    "                    print('# epoch {}: loss_tra = {}, loss_val = {}'.\n",
    "                          format(e, str(loss_tra), str(loss_val)))\n",
    "            \n",
    "            # モデルの保存\n",
    "            if save_model:\n",
    "                saver = tf.train.Saver()\n",
    "                _ = saver.save(sess, model_path)\n",
    "            \n",
    "            \n",
    "    def test(self, model_path='./prediction_model'):\n",
    "        \"\"\"指定されたモデルを読み込み，テストする関数です．\"\"\"\n",
    "        tf.reset_default_graph()\n",
    "        loaded_graph = tf.Graph()\n",
    "        \n",
    "        with tf.Session(graph=loaded_graph) as sess:\n",
    "            \n",
    "            # モデルの読み込み\n",
    "            loader = tf.train.import_meta_graph(\n",
    "                model_path + '.meta')\n",
    "            loader.restore(sess, model_path)\n",
    "            \n",
    "            x_loaded = loaded_graph.get_tensor_by_name('x:0')\n",
    "            y_loaded = loaded_graph.get_tensor_by_name('y:0')\n",
    "            \n",
    "            loss_loaded = loaded_graph.get_tensor_by_name('loss:0')\n",
    "            acc_loaded = loaded_graph.get_tensor_by_name('acc:0')\n",
    "            logits_loaded = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        \n",
    "            # test\n",
    "            feed_dict = {x_loaded: self.x_test, y_loaded: self.y_test}\n",
    "            loss_test, acc_test, logits_test = sess.run(\n",
    "                (loss_loaded, acc_loaded, logits_loaded), feed_dict=feed_dict)\n",
    "            return acc_test, logits_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "　以下では，簡単に`ComicNet`を解説します．TensorFlowの考え方については，[TensorFlowのキーコンセプト: Opノード、セッション、変数](http://qiita.com/yanosen_jp/items/70e6d6afc36e1c0a3ef3)が詳しいです．\n",
    "\n",
    "### データの生成：`get_x()`，`get_y()`，`get_xs_ys()`\n",
    "\n",
    "　`get_x()`は，0から1の間に正規化された掲載順を返します．`get_y()`は，短命作品か（`1`）否か（`0`）を返します．`get_xs_ys()`は，内部で`get_x()`および`get_y()`を呼び出し，複数タイトルの`x`および`y`を返します．\n",
    " \n",
    "　`ComicNet`は初期化時に，trainingデータ（`x_tra`，`y_tra`），validationデータ（`x_val`，`y_val`），およびtestデータ（`x_test`，`y_test`）を生成します．trainingデータは，文字通りモデルの学習に用いるデータです．検証用データは，モデルの汎化性能を一時的に評価するため（trainingデータへの過学習を防ぐため）のデータであり，ハイパーパラメータの調整時に利用します．testデータは，ハイパーパラメータ調整後のモデルの性能を評価するためのデータです．これらについては，[なぜ教師あり学習でバリデーションセットとテストセットを分ける必要があるのか？](http://qiita.com/QUANON/items/ae569961ea02b4468e23)がわかりやすいです．今回は，`wj.end_titles`のうち，最新100件をtestデータ，残りの最新`batch_size`件をvalidationデータ，残りをtrainデータとして用います．\n",
    "\n",
    "### ネットワークの構築：`build_graph()`\n",
    "\n",
    "　入力層では，`tf.placeholder`で入力テンソル（`x`）や教師ラベルテンソル（`y`）をを定義します．\n",
    " \n",
    "　隠れ層では，`tf.Variable`で重みテンソル（`w_h_1`，`w_h_2`，`w_h_3`）やバイアス（`b_h_1`，`b_h_2`，`b_h_3`）を定義します．ここでは，`Variable`の初期分布として[`tf.truncated_normal`](https://www.tensorflow.org/api_docs/python/tf/truncated_normal)を与えています．`truncated_normal`は，2シグマより外の値を除外した正規分布であり，好んでよく使われます．実は，この`truncated_normal`の標準偏差は，モデルの性能を左右する重要なハイパーパラメータの一つですが，今回は簡単のため`0.05`に固定します．[`tf.add`](https://www.tensorflow.org/api_docs/python/tf/add)，[`tf.matmul`](https://www.tensorflow.org/api_docs/python/tf/matmul)，[`tf.nn.relu`](https://www.tensorflow.org/api_docs/python/tf/nn/relu)を使って，テンソル同士を結合し，隠れ層を形作っていきます．ちなみに，[`tf.nn.relu`](https://www.tensorflow.org/api_docs/python/tf/nn/relu)を[`tf.nn.sigmoid`](https://www.tensorflow.org/api_docs/python/tf/sigmoid#tfnnsigmoid)に書き換れば，活性化関数として[Sigmoid](https://ja.wikipedia.org/wiki/%E6%B4%BB%E6%80%A7%E5%8C%96%E9%96%A2%E6%95%B0#.E3.82.B7.E3.82.B0.E3.83.A2.E3.82.A4.E3.83.89.E9.96.A2.E6.95.B0)を使うことができます．TensorFlowで使用可能な活性化関数については，[こちら](https://www.tensorflow.org/versions/r0.11/api_docs/python/nn/activation_functions_)をご参照ください．[`tf.summary.histgram`](https://www.tensorflow.org/api_docs/python/tf/summary/histogram)に`tf.Variable`を渡すことで，TensorBoardでヒストグラムを確認できるようになります．\n",
    "  \n",
    "　出力層では，基本的に潜れ層と同様の処理を行います．\n",
    " \n",
    "　コスト関数は，[`tf.nn.l2_loss`](https://www.tensorflow.org/versions/r0.11/api_docs/python/nn/losses)で定義します．[`tf.summary.scalar`](https://www.tensorflow.org/api_docs/python/tf/summary/scalar)に`tf.Variable`を渡すことで，TensorBoardで時変化を確認できるようになります．\n",
    " \n",
    "　最適化アルゴリズムとして，[`tf.train.AdamOptimizer`](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer)を使います．TensorFlowで使用可能な最適化アルゴリズムについては，[こちら](https://www.tensorflow.org/versions/r0.12/api_docs/python/train/)をご参照ください．最終的な出力値`logits`を四捨五入し（つまり閾値0.5で判定し），教師ラベル`y`に対する正解率を`acc`として計算しています．最後に，全ての[`tf.summary.merge_all`](https://www.tensorflow.org/api_docs/python/tf/summary/merge_all)で全てのログ情報をマージします．\n",
    "\n",
    "### 訓練：`train()`\n",
    "\n",
    "　TensorFlowでは，[`tf.Session`](https://www.tensorflow.org/api_docs/python/tf/Session)中で訓練を行います．必ず，[`tf.global_variables_initializer()`](https://www.tensorflow.org/api_docs/python/tf/global_variables_initializer)で`Variable`の初期化を行う必要があります（これがないと怒られます）．\n",
    " \n",
    " 　`sess.run(self.optimizer)`によって，モデルを訓練します．`sess.run`の第一引数は，タプルによって複数指定することが可能です．また，`sess.run()`時に，辞書形式で`placeholder`に値を代入する必要があります．Training時は`x_tra_batch`と`x_tra_batch`を代入し，Validation時は`x_val`と`y_val`を代入します．\n",
    "\n",
    "　[`tf.summary.FileWriter`](https://www.tensorflow.org/api_docs/python/tf/summary/FileWriter)で，TensorBoard用のログ情報を保存できます．また，[`tf.train.Saver`](https://www.tensorflow.org/api_docs/python/tf/train/Saver)で訓練後のモデルを保存できます．\n",
    "\n",
    "### テスト：`test()`\n",
    "\n",
    "　`test()`は，訓練した多層パーセプトロンをテストするメンバ関数です．[`tf.train.import_meta_graph`](https://www.tensorflow.org/api_docs/python/tf/train/import_meta_graph)を使って，学習済みのモデルを読み込みます．testデータ（`x_test`，`y_test`）を`feed_dict`に与え，`sess.run`します．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実験1.  短命作品（10週以内）の予測\n",
    "\n",
    "　当初の目的である短命作品（10週以内）の予測に挑戦します．\n",
    "\n",
    "### ハイパーパラメータの調整\n",
    "　[TensorBoard](https://www.tensorflow.org/get_started/summaries_and_tensorboard)を使ってハイパーパラメータ（隠れ層のノード数$n$と，学習率$r$）をチューニングします．TensorBoardの詳細は[公式](https://www.tensorflow.org/get_started/summaries_and_tensorboard)をご参照ください．いろいろ遊べますが，本記事では，validationデータの正解率とコスト関数を可視化することで，ハイパーパラメータの調整を行います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved log of model=1, r=0.1, n=1\n",
      "Saved log of model=1, r=0.1, n=2\n",
      "Saved log of model=1, r=0.1, n=3\n",
      "Saved log of model=1, r=0.1, n=4\n",
      "Saved log of model=1, r=0.1, n=5\n",
      "Saved log of model=1, r=0.1, n=6\n",
      "Saved log of model=1, r=0.1, n=7\n",
      "Saved log of model=1, r=0.05, n=1\n",
      "Saved log of model=1, r=0.05, n=2\n",
      "Saved log of model=1, r=0.05, n=3\n",
      "Saved log of model=1, r=0.05, n=4\n",
      "Saved log of model=1, r=0.05, n=5\n",
      "Saved log of model=1, r=0.05, n=6\n",
      "Saved log of model=1, r=0.05, n=7\n",
      "Saved log of model=1, r=0.01, n=1\n",
      "Saved log of model=1, r=0.01, n=2\n",
      "Saved log of model=1, r=0.01, n=3\n",
      "Saved log of model=1, r=0.01, n=4\n",
      "Saved log of model=1, r=0.01, n=5\n",
      "Saved log of model=1, r=0.01, n=6\n",
      "Saved log of model=1, r=0.01, n=7\n",
      "Saved log of model=1, r=0.005, n=1\n",
      "Saved log of model=1, r=0.005, n=2\n",
      "Saved log of model=1, r=0.005, n=3\n",
      "Saved log of model=1, r=0.005, n=4\n",
      "Saved log of model=1, r=0.005, n=5\n",
      "Saved log of model=1, r=0.005, n=6\n",
      "Saved log of model=1, r=0.005, n=7\n",
      "Saved log of model=1, r=0.001, n=1\n",
      "Saved log of model=1, r=0.001, n=2\n",
      "Saved log of model=1, r=0.001, n=3\n",
      "Saved log of model=1, r=0.001, n=4\n",
      "Saved log of model=1, r=0.001, n=5\n",
      "Saved log of model=1, r=0.001, n=6\n",
      "Saved log of model=1, r=0.001, n=7\n",
      "Saved log of model=2, r=0.1, n=1\n",
      "Saved log of model=2, r=0.1, n=2\n",
      "Saved log of model=2, r=0.1, n=3\n",
      "Saved log of model=2, r=0.1, n=4\n",
      "Saved log of model=2, r=0.1, n=5\n",
      "Saved log of model=2, r=0.1, n=6\n",
      "Saved log of model=2, r=0.1, n=7\n",
      "Saved log of model=2, r=0.05, n=1\n",
      "Saved log of model=2, r=0.05, n=2\n",
      "Saved log of model=2, r=0.05, n=3\n",
      "Saved log of model=2, r=0.05, n=4\n",
      "Saved log of model=2, r=0.05, n=5\n",
      "Saved log of model=2, r=0.05, n=6\n",
      "Saved log of model=2, r=0.05, n=7\n",
      "Saved log of model=2, r=0.01, n=1\n",
      "Saved log of model=2, r=0.01, n=2\n",
      "Saved log of model=2, r=0.01, n=3\n",
      "Saved log of model=2, r=0.01, n=4\n",
      "Saved log of model=2, r=0.01, n=5\n",
      "Saved log of model=2, r=0.01, n=6\n",
      "Saved log of model=2, r=0.01, n=7\n",
      "Saved log of model=2, r=0.005, n=1\n",
      "Saved log of model=2, r=0.005, n=2\n",
      "Saved log of model=2, r=0.005, n=3\n",
      "Saved log of model=2, r=0.005, n=4\n",
      "Saved log of model=2, r=0.005, n=5\n",
      "Saved log of model=2, r=0.005, n=6\n",
      "Saved log of model=2, r=0.005, n=7\n",
      "Saved log of model=2, r=0.001, n=1\n",
      "Saved log of model=2, r=0.001, n=2\n",
      "Saved log of model=2, r=0.001, n=3\n",
      "Saved log of model=2, r=0.001, n=4\n",
      "Saved log of model=2, r=0.001, n=5\n",
      "Saved log of model=2, r=0.001, n=6\n",
      "Saved log of model=2, r=0.001, n=7\n",
      "Saved log of model=3, r=0.1, n=1\n",
      "Saved log of model=3, r=0.1, n=2\n",
      "Saved log of model=3, r=0.1, n=3\n",
      "Saved log of model=3, r=0.1, n=4\n",
      "Saved log of model=3, r=0.1, n=5\n",
      "Saved log of model=3, r=0.1, n=6\n",
      "Saved log of model=3, r=0.1, n=7\n",
      "Saved log of model=3, r=0.05, n=1\n",
      "Saved log of model=3, r=0.05, n=2\n",
      "Saved log of model=3, r=0.05, n=3\n",
      "Saved log of model=3, r=0.05, n=4\n",
      "Saved log of model=3, r=0.05, n=5\n",
      "Saved log of model=3, r=0.05, n=6\n",
      "Saved log of model=3, r=0.05, n=7\n",
      "Saved log of model=3, r=0.01, n=1\n",
      "Saved log of model=3, r=0.01, n=2\n",
      "Saved log of model=3, r=0.01, n=3\n",
      "Saved log of model=3, r=0.01, n=4\n",
      "Saved log of model=3, r=0.01, n=5\n",
      "Saved log of model=3, r=0.01, n=6\n",
      "Saved log of model=3, r=0.01, n=7\n",
      "Saved log of model=3, r=0.005, n=1\n",
      "Saved log of model=3, r=0.005, n=2\n",
      "Saved log of model=3, r=0.005, n=3\n",
      "Saved log of model=3, r=0.005, n=4\n",
      "Saved log of model=3, r=0.005, n=5\n",
      "Saved log of model=3, r=0.005, n=6\n",
      "Saved log of model=3, r=0.005, n=7\n",
      "Saved log of model=3, r=0.001, n=1\n",
      "Saved log of model=3, r=0.001, n=2\n",
      "Saved log of model=3, r=0.001, n=3\n",
      "Saved log of model=3, r=0.001, n=4\n",
      "Saved log of model=3, r=0.001, n=5\n",
      "Saved log of model=3, r=0.001, n=6\n",
      "Saved log of model=3, r=0.001, n=7\n",
      "Saved log of model=4, r=0.1, n=1\n",
      "Saved log of model=4, r=0.1, n=2\n",
      "Saved log of model=4, r=0.1, n=3\n",
      "Saved log of model=4, r=0.1, n=4\n",
      "Saved log of model=4, r=0.1, n=5\n",
      "Saved log of model=4, r=0.1, n=6\n",
      "Saved log of model=4, r=0.1, n=7\n",
      "Saved log of model=4, r=0.05, n=1\n",
      "Saved log of model=4, r=0.05, n=2\n",
      "Saved log of model=4, r=0.05, n=3\n",
      "Saved log of model=4, r=0.05, n=4\n",
      "Saved log of model=4, r=0.05, n=5\n",
      "Saved log of model=4, r=0.05, n=6\n",
      "Saved log of model=4, r=0.05, n=7\n",
      "Saved log of model=4, r=0.01, n=1\n",
      "Saved log of model=4, r=0.01, n=2\n",
      "Saved log of model=4, r=0.01, n=3\n",
      "Saved log of model=4, r=0.01, n=4\n",
      "Saved log of model=4, r=0.01, n=5\n",
      "Saved log of model=4, r=0.01, n=6\n",
      "Saved log of model=4, r=0.01, n=7\n",
      "Saved log of model=4, r=0.005, n=1\n",
      "Saved log of model=4, r=0.005, n=2\n",
      "Saved log of model=4, r=0.005, n=3\n",
      "Saved log of model=4, r=0.005, n=4\n",
      "Saved log of model=4, r=0.005, n=5\n",
      "Saved log of model=4, r=0.005, n=6\n",
      "Saved log of model=4, r=0.005, n=7\n",
      "Saved log of model=4, r=0.001, n=1\n",
      "Saved log of model=4, r=0.001, n=2\n",
      "Saved log of model=4, r=0.001, n=3\n",
      "Saved log of model=4, r=0.001, n=4\n",
      "Saved log of model=4, r=0.001, n=5\n",
      "Saved log of model=4, r=0.001, n=6\n",
      "Saved log of model=4, r=0.001, n=7\n",
      "Saved log of model=5, r=0.1, n=1\n",
      "Saved log of model=5, r=0.1, n=2\n",
      "Saved log of model=5, r=0.1, n=3\n",
      "Saved log of model=5, r=0.1, n=4\n",
      "Saved log of model=5, r=0.1, n=5\n",
      "Saved log of model=5, r=0.1, n=6\n",
      "Saved log of model=5, r=0.1, n=7\n",
      "Saved log of model=5, r=0.05, n=1\n",
      "Saved log of model=5, r=0.05, n=2\n",
      "Saved log of model=5, r=0.05, n=3\n",
      "Saved log of model=5, r=0.05, n=4\n",
      "Saved log of model=5, r=0.05, n=5\n",
      "Saved log of model=5, r=0.05, n=6\n",
      "Saved log of model=5, r=0.05, n=7\n",
      "Saved log of model=5, r=0.01, n=1\n",
      "Saved log of model=5, r=0.01, n=2\n",
      "Saved log of model=5, r=0.01, n=3\n",
      "Saved log of model=5, r=0.01, n=4\n",
      "Saved log of model=5, r=0.01, n=5\n",
      "Saved log of model=5, r=0.01, n=6\n",
      "Saved log of model=5, r=0.01, n=7\n",
      "Saved log of model=5, r=0.005, n=1\n",
      "Saved log of model=5, r=0.005, n=2\n",
      "Saved log of model=5, r=0.005, n=3\n",
      "Saved log of model=5, r=0.005, n=4\n",
      "Saved log of model=5, r=0.005, n=5\n",
      "Saved log of model=5, r=0.005, n=6\n",
      "Saved log of model=5, r=0.005, n=7\n",
      "Saved log of model=5, r=0.001, n=1\n",
      "Saved log of model=5, r=0.001, n=2\n",
      "Saved log of model=5, r=0.001, n=3\n",
      "Saved log of model=5, r=0.001, n=4\n",
      "Saved log of model=5, r=0.001, n=5\n",
      "Saved log of model=5, r=0.001, n=6\n",
      "Saved log of model=5, r=0.001, n=7\n",
      "Saved log of model=6, r=0.1, n=1\n",
      "Saved log of model=6, r=0.1, n=2\n",
      "Saved log of model=6, r=0.1, n=3\n",
      "Saved log of model=6, r=0.1, n=4\n",
      "Saved log of model=6, r=0.1, n=5\n",
      "Saved log of model=6, r=0.1, n=6\n",
      "Saved log of model=6, r=0.1, n=7\n",
      "Saved log of model=6, r=0.05, n=1\n",
      "Saved log of model=6, r=0.05, n=2\n",
      "Saved log of model=6, r=0.05, n=3\n",
      "Saved log of model=6, r=0.05, n=4\n",
      "Saved log of model=6, r=0.05, n=5\n",
      "Saved log of model=6, r=0.05, n=6\n",
      "Saved log of model=6, r=0.05, n=7\n",
      "Saved log of model=6, r=0.01, n=1\n",
      "Saved log of model=6, r=0.01, n=2\n",
      "Saved log of model=6, r=0.01, n=3\n",
      "Saved log of model=6, r=0.01, n=4\n",
      "Saved log of model=6, r=0.01, n=5\n",
      "Saved log of model=6, r=0.01, n=6\n",
      "Saved log of model=6, r=0.01, n=7\n",
      "Saved log of model=6, r=0.005, n=1\n",
      "Saved log of model=6, r=0.005, n=2\n",
      "Saved log of model=6, r=0.005, n=3\n",
      "Saved log of model=6, r=0.005, n=4\n",
      "Saved log of model=6, r=0.005, n=5\n",
      "Saved log of model=6, r=0.005, n=6\n",
      "Saved log of model=6, r=0.005, n=7\n",
      "Saved log of model=6, r=0.001, n=1\n",
      "Saved log of model=6, r=0.001, n=2\n",
      "Saved log of model=6, r=0.001, n=3\n",
      "Saved log of model=6, r=0.001, n=4\n",
      "Saved log of model=6, r=0.001, n=5\n",
      "Saved log of model=6, r=0.001, n=6\n",
      "Saved log of model=6, r=0.001, n=7\n",
      "Saved log of model=7, r=0.1, n=1\n",
      "Saved log of model=7, r=0.1, n=2\n",
      "Saved log of model=7, r=0.1, n=3\n",
      "Saved log of model=7, r=0.1, n=4\n",
      "Saved log of model=7, r=0.1, n=5\n",
      "Saved log of model=7, r=0.1, n=6\n",
      "Saved log of model=7, r=0.1, n=7\n",
      "Saved log of model=7, r=0.05, n=1\n",
      "Saved log of model=7, r=0.05, n=2\n",
      "Saved log of model=7, r=0.05, n=3\n",
      "Saved log of model=7, r=0.05, n=4\n",
      "Saved log of model=7, r=0.05, n=5\n",
      "Saved log of model=7, r=0.05, n=6\n",
      "Saved log of model=7, r=0.05, n=7\n",
      "Saved log of model=7, r=0.01, n=1\n",
      "Saved log of model=7, r=0.01, n=2\n",
      "Saved log of model=7, r=0.01, n=3\n",
      "Saved log of model=7, r=0.01, n=4\n",
      "Saved log of model=7, r=0.01, n=5\n",
      "Saved log of model=7, r=0.01, n=6\n",
      "Saved log of model=7, r=0.01, n=7\n",
      "Saved log of model=7, r=0.005, n=1\n",
      "Saved log of model=7, r=0.005, n=2\n",
      "Saved log of model=7, r=0.005, n=3\n",
      "Saved log of model=7, r=0.005, n=4\n",
      "Saved log of model=7, r=0.005, n=5\n",
      "Saved log of model=7, r=0.005, n=6\n",
      "Saved log of model=7, r=0.005, n=7\n",
      "Saved log of model=7, r=0.001, n=1\n",
      "Saved log of model=7, r=0.001, n=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved log of model=7, r=0.001, n=3\n",
      "Saved log of model=7, r=0.001, n=4\n",
      "Saved log of model=7, r=0.001, n=5\n",
      "Saved log of model=7, r=0.001, n=6\n",
      "Saved log of model=7, r=0.001, n=7\n"
     ]
    }
   ],
   "source": [
    "for model in range(1, 8):\n",
    "    wjnet = ComicNet(wj, model=model)\n",
    "    for r in [0.1, 0.05, 0.01, 0.005, 0.001]:\n",
    "        for n in range(1, 8):\n",
    "            log_dir = './logs/1/model=' + str(model)\n",
    "            log_name = 'r={},n={}'.format(str(r), str(n))\n",
    "            wjnet.build_graph(r=r, n_h=n)\n",
    "            wjnet.train(save_log=True, log_dir=log_dir, log_name=log_name)\n",
    "            print('Saved log of model={}, r={}, n={}'.\\\n",
    "                  format(str(model), str(r), str(n)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "　TensorBoardを起動します．ここでは，`model=7`のvalidation結果を見てみます．\n",
    " \n",
    "```bash\n",
    "tensorboard --logdir=./logs/1/model=7/val\n",
    "```\n",
    "\n",
    "　指定されたURLをブラウザで開きます．\n",
    "\n",
    "![tensorboard.png](fig/tensorboard.png)\n",
    "\n",
    "　上図はvalidationデータの正解率（`acc`），下図はvalidationデータのコスト関数（`loss`）を表します．横軸はエポック数を表します．凡例が多すぎてわけがわからないので，左下の`Runs`でフィルタリングします．下図は，`n=7`でフィルタリングした例です．\n",
    "\n",
    "![scalar,n=7.png](fig/scalar,n=7.png)\n",
    "\n",
    "　`r=0.001`がいい感じであることがわかります．次は，`r=0.001`でフィルタリングします．\n",
    "\n",
    "![scalar,r=0.001.png](fig/scalar,r=0.001.png)\n",
    "\n",
    "　どうやら，`model=7`に関しては，`n=7, r=0.001`が良さそうです．また，`e=5000`あたりからtrainingデータへの過学習が始まり，正解率とコスト関数が悪化していることがなんとなくわかります．そこで，`model=7`に関しては，学習を5000エポックで打ち切ることにします．\n",
    " \n",
    "　上記の作業を，他のモデル（`model=1, 2,..., 6`）に関しても行い，ハイパーパラメータを最適化します．\n",
    " \n",
    " |model | $r$ | $n$ | epoch |\n",
    " |:--|:--|:--|:--|\n",
    " |1|0.001|1|5000|\n",
    " |2|0.05|3|5000|\n",
    " |3|0.001|5|2000|\n",
    " |4|0.005|7|5000|\n",
    " |5|0.01|7|5000|\n",
    " |6|0.001|7|3000|\n",
    " |7|0.001|7|5000|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習\n",
    "\n",
    "　最適化したハイパーパラメータで，再度訓練を行い，モデルを保存します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained ./model1_thesh10\n",
      "Trained ./model2_thesh10\n",
      "Trained ./model3_thesh10\n"
     ]
    }
   ],
   "source": [
    "params = [\n",
    "    [1, 0.001, 1, 5000], [2, 0.05, 3, 5000], [3, 0.001, 5, 2000],\n",
    "    [4, 0.005, 7, 5000], [5, 0.01, 7, 5000], [6, 0.001, 7, 3000], \n",
    "    [7, 0.001, 7, 5000]]\n",
    "for param in params:\n",
    "    model = param[0]\n",
    "    r = param[1]\n",
    "    n = param[2]\n",
    "    epoch = param[3]\n",
    "    model_path = './model{}_thesh10'.format(str(model))\n",
    "    \n",
    "    wjnet = ComicNet(wj, model=model)\n",
    "    wjnet.build_graph(r=r, n_h=n)\n",
    "    wjnet.train(save_model=True, model_path=model_path, epoch=epoch)\n",
    "    print('Trained', model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テスト"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 考察"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f7f7affd0b8>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxMAAAIfCAYAAAAL74GfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBAAAMMn/D8m2kG2gKtDnAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7f7c21d828>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 271,
       "width": 393
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxgAAAIfCAYAAADzD3poAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7f7c245ac8>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 271,
       "width": 396
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "weeks = [len(wj.extract_item(title))\n",
    "         for title in wj.end_titles]\n",
    "plt.figure()\n",
    "_ = plt.hist(weeks, normed=True, cumulative=True, bins=max(weeks))\n",
    "plt.xlabel('Serialized weeks')\n",
    "plt.ylabel('CDF')\n",
    "\n",
    "plt.figure()\n",
    "_ = plt.hist(weeks, normed=True, cumulative=True, bins=max(weeks))\n",
    "plt.xlim(0, 50)\n",
    "plt.xlabel('Serialized weeks')\n",
    "plt.ylabel('CDF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実験2. 短命作品（20週以内）の予測\n",
    "\n",
    "### ハイパーパラメータの調整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved log of model=1, r=0.1, n=1\n",
      "Saved log of model=1, r=0.1, n=2\n",
      "Saved log of model=1, r=0.1, n=3\n",
      "Saved log of model=1, r=0.1, n=4\n",
      "Saved log of model=1, r=0.1, n=5\n",
      "Saved log of model=1, r=0.1, n=6\n",
      "Saved log of model=1, r=0.1, n=7\n",
      "Saved log of model=1, r=0.05, n=1\n",
      "Saved log of model=1, r=0.05, n=2\n",
      "Saved log of model=1, r=0.05, n=3\n",
      "Saved log of model=1, r=0.05, n=4\n",
      "Saved log of model=1, r=0.05, n=5\n",
      "Saved log of model=1, r=0.05, n=6\n",
      "Saved log of model=1, r=0.05, n=7\n",
      "Saved log of model=1, r=0.01, n=1\n",
      "Saved log of model=1, r=0.01, n=2\n",
      "Saved log of model=1, r=0.01, n=3\n",
      "Saved log of model=1, r=0.01, n=4\n",
      "Saved log of model=1, r=0.01, n=5\n",
      "Saved log of model=1, r=0.01, n=6\n",
      "Saved log of model=1, r=0.01, n=7\n",
      "Saved log of model=1, r=0.005, n=1\n",
      "Saved log of model=1, r=0.005, n=2\n",
      "Saved log of model=1, r=0.005, n=3\n",
      "Saved log of model=1, r=0.005, n=4\n",
      "Saved log of model=1, r=0.005, n=5\n",
      "Saved log of model=1, r=0.005, n=6\n",
      "Saved log of model=1, r=0.005, n=7\n",
      "Saved log of model=1, r=0.001, n=1\n",
      "Saved log of model=1, r=0.001, n=2\n",
      "Saved log of model=1, r=0.001, n=3\n",
      "Saved log of model=1, r=0.001, n=4\n",
      "Saved log of model=1, r=0.001, n=5\n",
      "Saved log of model=1, r=0.001, n=6\n",
      "Saved log of model=1, r=0.001, n=7\n",
      "Saved log of model=2, r=0.1, n=1\n",
      "Saved log of model=2, r=0.1, n=2\n",
      "Saved log of model=2, r=0.1, n=3\n",
      "Saved log of model=2, r=0.1, n=4\n",
      "Saved log of model=2, r=0.1, n=5\n",
      "Saved log of model=2, r=0.1, n=6\n",
      "Saved log of model=2, r=0.1, n=7\n",
      "Saved log of model=2, r=0.05, n=1\n",
      "Saved log of model=2, r=0.05, n=2\n",
      "Saved log of model=2, r=0.05, n=3\n",
      "Saved log of model=2, r=0.05, n=4\n",
      "Saved log of model=2, r=0.05, n=5\n",
      "Saved log of model=2, r=0.05, n=6\n",
      "Saved log of model=2, r=0.05, n=7\n",
      "Saved log of model=2, r=0.01, n=1\n",
      "Saved log of model=2, r=0.01, n=2\n",
      "Saved log of model=2, r=0.01, n=3\n",
      "Saved log of model=2, r=0.01, n=4\n",
      "Saved log of model=2, r=0.01, n=5\n",
      "Saved log of model=2, r=0.01, n=6\n",
      "Saved log of model=2, r=0.01, n=7\n",
      "Saved log of model=2, r=0.005, n=1\n",
      "Saved log of model=2, r=0.005, n=2\n",
      "Saved log of model=2, r=0.005, n=3\n",
      "Saved log of model=2, r=0.005, n=4\n",
      "Saved log of model=2, r=0.005, n=5\n",
      "Saved log of model=2, r=0.005, n=6\n",
      "Saved log of model=2, r=0.005, n=7\n",
      "Saved log of model=2, r=0.001, n=1\n",
      "Saved log of model=2, r=0.001, n=2\n",
      "Saved log of model=2, r=0.001, n=3\n",
      "Saved log of model=2, r=0.001, n=4\n",
      "Saved log of model=2, r=0.001, n=5\n",
      "Saved log of model=2, r=0.001, n=6\n",
      "Saved log of model=2, r=0.001, n=7\n",
      "Saved log of model=3, r=0.1, n=1\n",
      "Saved log of model=3, r=0.1, n=2\n",
      "Saved log of model=3, r=0.1, n=3\n",
      "Saved log of model=3, r=0.1, n=4\n",
      "Saved log of model=3, r=0.1, n=5\n",
      "Saved log of model=3, r=0.1, n=6\n",
      "Saved log of model=3, r=0.1, n=7\n",
      "Saved log of model=3, r=0.05, n=1\n",
      "Saved log of model=3, r=0.05, n=2\n",
      "Saved log of model=3, r=0.05, n=3\n",
      "Saved log of model=3, r=0.05, n=4\n",
      "Saved log of model=3, r=0.05, n=5\n",
      "Saved log of model=3, r=0.05, n=6\n",
      "Saved log of model=3, r=0.05, n=7\n",
      "Saved log of model=3, r=0.01, n=1\n",
      "Saved log of model=3, r=0.01, n=2\n",
      "Saved log of model=3, r=0.01, n=3\n",
      "Saved log of model=3, r=0.01, n=4\n",
      "Saved log of model=3, r=0.01, n=5\n",
      "Saved log of model=3, r=0.01, n=6\n",
      "Saved log of model=3, r=0.01, n=7\n",
      "Saved log of model=3, r=0.005, n=1\n",
      "Saved log of model=3, r=0.005, n=2\n",
      "Saved log of model=3, r=0.005, n=3\n",
      "Saved log of model=3, r=0.005, n=4\n",
      "Saved log of model=3, r=0.005, n=5\n",
      "Saved log of model=3, r=0.005, n=6\n",
      "Saved log of model=3, r=0.005, n=7\n",
      "Saved log of model=3, r=0.001, n=1\n",
      "Saved log of model=3, r=0.001, n=2\n",
      "Saved log of model=3, r=0.001, n=3\n",
      "Saved log of model=3, r=0.001, n=4\n",
      "Saved log of model=3, r=0.001, n=5\n",
      "Saved log of model=3, r=0.001, n=6\n",
      "Saved log of model=3, r=0.001, n=7\n",
      "Saved log of model=4, r=0.1, n=1\n",
      "Saved log of model=4, r=0.1, n=2\n",
      "Saved log of model=4, r=0.1, n=3\n",
      "Saved log of model=4, r=0.1, n=4\n",
      "Saved log of model=4, r=0.1, n=5\n",
      "Saved log of model=4, r=0.1, n=6\n",
      "Saved log of model=4, r=0.1, n=7\n",
      "Saved log of model=4, r=0.05, n=1\n",
      "Saved log of model=4, r=0.05, n=2\n",
      "Saved log of model=4, r=0.05, n=3\n",
      "Saved log of model=4, r=0.05, n=4\n",
      "Saved log of model=4, r=0.05, n=5\n",
      "Saved log of model=4, r=0.05, n=6\n",
      "Saved log of model=4, r=0.05, n=7\n",
      "Saved log of model=4, r=0.01, n=1\n",
      "Saved log of model=4, r=0.01, n=2\n",
      "Saved log of model=4, r=0.01, n=3\n",
      "Saved log of model=4, r=0.01, n=4\n",
      "Saved log of model=4, r=0.01, n=5\n",
      "Saved log of model=4, r=0.01, n=6\n",
      "Saved log of model=4, r=0.01, n=7\n",
      "Saved log of model=4, r=0.005, n=1\n",
      "Saved log of model=4, r=0.005, n=2\n",
      "Saved log of model=4, r=0.005, n=3\n",
      "Saved log of model=4, r=0.005, n=4\n",
      "Saved log of model=4, r=0.005, n=5\n",
      "Saved log of model=4, r=0.005, n=6\n",
      "Saved log of model=4, r=0.005, n=7\n",
      "Saved log of model=4, r=0.001, n=1\n",
      "Saved log of model=4, r=0.001, n=2\n",
      "Saved log of model=4, r=0.001, n=3\n",
      "Saved log of model=4, r=0.001, n=4\n",
      "Saved log of model=4, r=0.001, n=5\n",
      "Saved log of model=4, r=0.001, n=6\n",
      "Saved log of model=4, r=0.001, n=7\n",
      "Saved log of model=5, r=0.1, n=1\n",
      "Saved log of model=5, r=0.1, n=2\n",
      "Saved log of model=5, r=0.1, n=3\n",
      "Saved log of model=5, r=0.1, n=4\n",
      "Saved log of model=5, r=0.1, n=5\n",
      "Saved log of model=5, r=0.1, n=6\n",
      "Saved log of model=5, r=0.1, n=7\n",
      "Saved log of model=5, r=0.05, n=1\n",
      "Saved log of model=5, r=0.05, n=2\n",
      "Saved log of model=5, r=0.05, n=3\n",
      "Saved log of model=5, r=0.05, n=4\n",
      "Saved log of model=5, r=0.05, n=5\n",
      "Saved log of model=5, r=0.05, n=6\n",
      "Saved log of model=5, r=0.05, n=7\n",
      "Saved log of model=5, r=0.01, n=1\n",
      "Saved log of model=5, r=0.01, n=2\n",
      "Saved log of model=5, r=0.01, n=3\n",
      "Saved log of model=5, r=0.01, n=4\n",
      "Saved log of model=5, r=0.01, n=5\n",
      "Saved log of model=5, r=0.01, n=6\n",
      "Saved log of model=5, r=0.01, n=7\n",
      "Saved log of model=5, r=0.005, n=1\n",
      "Saved log of model=5, r=0.005, n=2\n",
      "Saved log of model=5, r=0.005, n=3\n",
      "Saved log of model=5, r=0.005, n=4\n",
      "Saved log of model=5, r=0.005, n=5\n",
      "Saved log of model=5, r=0.005, n=6\n",
      "Saved log of model=5, r=0.005, n=7\n",
      "Saved log of model=5, r=0.001, n=1\n",
      "Saved log of model=5, r=0.001, n=2\n",
      "Saved log of model=5, r=0.001, n=3\n",
      "Saved log of model=5, r=0.001, n=4\n",
      "Saved log of model=5, r=0.001, n=5\n",
      "Saved log of model=5, r=0.001, n=6\n",
      "Saved log of model=5, r=0.001, n=7\n",
      "Saved log of model=6, r=0.1, n=1\n",
      "Saved log of model=6, r=0.1, n=2\n",
      "Saved log of model=6, r=0.1, n=3\n",
      "Saved log of model=6, r=0.1, n=4\n",
      "Saved log of model=6, r=0.1, n=5\n",
      "Saved log of model=6, r=0.1, n=6\n",
      "Saved log of model=6, r=0.1, n=7\n",
      "Saved log of model=6, r=0.05, n=1\n",
      "Saved log of model=6, r=0.05, n=2\n",
      "Saved log of model=6, r=0.05, n=3\n",
      "Saved log of model=6, r=0.05, n=4\n",
      "Saved log of model=6, r=0.05, n=5\n",
      "Saved log of model=6, r=0.05, n=6\n",
      "Saved log of model=6, r=0.05, n=7\n",
      "Saved log of model=6, r=0.01, n=1\n",
      "Saved log of model=6, r=0.01, n=2\n",
      "Saved log of model=6, r=0.01, n=3\n",
      "Saved log of model=6, r=0.01, n=4\n",
      "Saved log of model=6, r=0.01, n=5\n",
      "Saved log of model=6, r=0.01, n=6\n",
      "Saved log of model=6, r=0.01, n=7\n",
      "Saved log of model=6, r=0.005, n=1\n",
      "Saved log of model=6, r=0.005, n=2\n",
      "Saved log of model=6, r=0.005, n=3\n",
      "Saved log of model=6, r=0.005, n=4\n",
      "Saved log of model=6, r=0.005, n=5\n",
      "Saved log of model=6, r=0.005, n=6\n",
      "Saved log of model=6, r=0.005, n=7\n",
      "Saved log of model=6, r=0.001, n=1\n",
      "Saved log of model=6, r=0.001, n=2\n",
      "Saved log of model=6, r=0.001, n=3\n",
      "Saved log of model=6, r=0.001, n=4\n",
      "Saved log of model=6, r=0.001, n=5\n",
      "Saved log of model=6, r=0.001, n=6\n",
      "Saved log of model=6, r=0.001, n=7\n",
      "Saved log of model=7, r=0.1, n=1\n",
      "Saved log of model=7, r=0.1, n=2\n",
      "Saved log of model=7, r=0.1, n=3\n",
      "Saved log of model=7, r=0.1, n=4\n",
      "Saved log of model=7, r=0.1, n=5\n",
      "Saved log of model=7, r=0.1, n=6\n",
      "Saved log of model=7, r=0.1, n=7\n",
      "Saved log of model=7, r=0.05, n=1\n",
      "Saved log of model=7, r=0.05, n=2\n",
      "Saved log of model=7, r=0.05, n=3\n",
      "Saved log of model=7, r=0.05, n=4\n",
      "Saved log of model=7, r=0.05, n=5\n",
      "Saved log of model=7, r=0.05, n=6\n",
      "Saved log of model=7, r=0.05, n=7\n",
      "Saved log of model=7, r=0.01, n=1\n",
      "Saved log of model=7, r=0.01, n=2\n",
      "Saved log of model=7, r=0.01, n=3\n",
      "Saved log of model=7, r=0.01, n=4\n",
      "Saved log of model=7, r=0.01, n=5\n",
      "Saved log of model=7, r=0.01, n=6\n",
      "Saved log of model=7, r=0.01, n=7\n",
      "Saved log of model=7, r=0.005, n=1\n",
      "Saved log of model=7, r=0.005, n=2\n",
      "Saved log of model=7, r=0.005, n=3\n",
      "Saved log of model=7, r=0.005, n=4\n",
      "Saved log of model=7, r=0.005, n=5\n",
      "Saved log of model=7, r=0.005, n=6\n",
      "Saved log of model=7, r=0.005, n=7\n",
      "Saved log of model=7, r=0.001, n=1\n",
      "Saved log of model=7, r=0.001, n=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved log of model=7, r=0.001, n=3\n",
      "Saved log of model=7, r=0.001, n=4\n",
      "Saved log of model=7, r=0.001, n=5\n",
      "Saved log of model=7, r=0.001, n=6\n",
      "Saved log of model=7, r=0.001, n=7\n"
     ]
    }
   ],
   "source": [
    "for model in range(1, 8):\n",
    "    wjnet = ComicNet(wj, model=model, thresh_week=20)\n",
    "    for r in [0.1, 0.05, 0.01, 0.005, 0.001]:\n",
    "        for n in range(1, 8):\n",
    "            log_dir = './logs/2/model=' + str(model)\n",
    "            log_name = 'r={},n={}'.format(str(r), str(n))\n",
    "            wjnet.build_graph(r=r, n_h=n)\n",
    "            wjnet.train(save_log=True, log_dir=log_dir, log_name=log_name)\n",
    "            print('Saved log of model={}, r={}, n={}'.\\\n",
    "                  format(str(model), str(r), str(n)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テスト"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 考察"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実験3. 架空のマンガ作品の打切予測"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## タスク（2017/5/21）\n",
    "\n",
    "* `ComicNet()`のメンバ関数に，データ生成用の関数も含めること．`ComicNet()`には，何も考えずにComicAnalizer()`の結果を突っ込めるようにしたい．\n",
    "* `ComicNet()`の引数に，`threash_week`を追加すること．\n",
    "* `ComicNet()`の引数に，`save`を追加すること．これは，学習結果を保存するか否かを表すフラグ．\n",
    "* 最終的には，10週終了，20週終了，30週終了の予測結果をまとめてひとつのグラフにすること．\n",
    "* 考察では，大きく外れた例を一つか２つ掲載すること．\n",
    "* `ComicEndPredictor()`というクラスを生成すること．これは，学習済みデータをそのまま使って，終了予測を行うことができるクラス．つまり，githubには学習済みクラスのsavarデータをアップする必要がある．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
