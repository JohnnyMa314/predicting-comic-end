{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 短命作品の学習と予測\n",
    "\n",
    "　目次情報を学習して，短命作品を予測してみます．\n",
    "\n",
    "## 環境構築\n",
    "\n",
    "```bash\n",
    "conda env create -f env.yml\n",
    "```\n",
    "\n",
    "## 問題設定\n",
    "\n",
    "　本記事の目的は，2週目から7週目までの掲載順を入力とし，当該作品が短命作品（10週以内に終了する作品）か否かを予測することです．1週目の掲載順を除外したのは，新連載作品はほとんど巻頭に掲載されるためです．また，近年最短とされる作品が8週連載なので，遅くともこの1週間前には打ち切りを予測するため，7週目までの掲載順を入力としました．打ち切りを阻止するためには，もっと早期に打ち切りを予測する必要がありますが，これは今後の課題ということで…．\n",
    "\n",
    "## 目次データ\n",
    "\n",
    "　[0_obtain_comic_data_j.ipynb](0_obtain_comic_data_j.ipynb)で取得した`data/wj-api.json`を使います．また，[1_analyze_comic_data_j.ipynb](1_analyze_comic_data_j.ipynb)で定義した`ComicAnalyzer`を`analyze.py`からimportして使います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import analize\n",
    "\n",
    "wj = analize.ComicAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル\n",
    "\n",
    "　以下に，本記事で想定する[多層パーセプトロン](https://en.wikipedia.org/wiki/Multilayer_perceptron)のモデルを示します．多層パーセプトロンについては，[誤差逆伝播法のノート](http://qiita.com/Ugo-Nama/items/04814a13c9ea84978a4c)が詳しいです．\n",
    "\n",
    "![model.png](fig/model.png)\n",
    "\n",
    "　本記事では，隠れ層1層の多層パーセプトロンを用います．入力層は6ノード（2週目から7週目までの掲載順），隠れ層は$n$ノード，出力層は1ノード（短命作品である確率）を含みます．隠れ層および出力層の活性化関数として，[Sigmoid関数](https://ja.wikipedia.org/wiki/%E6%B4%BB%E6%80%A7%E5%8C%96%E9%96%A2%E6%95%B0#.E3.82.B7.E3.82.B0.E3.83.A2.E3.82.A4.E3.83.89.E9.96.A2.E6.95.B0)を用います．学習率として，$r$を用います．最適化アルゴリズムとして，オーソドックスな[Stochastic Gradient Descent (SGD)](https://ja.wikipedia.org/wiki/%E7%A2%BA%E7%8E%87%E7%9A%84%E5%8B%BE%E9%85%8D%E9%99%8D%E4%B8%8B%E6%B3%95#cite_note-17)と，人気の[Adam](https://arxiv.org/abs/1412.6980)の両方を使って結果を比較します．\n",
    " \n",
    "　本記事では，以下のハイパーパラメータを調整して，予測性能の最大化を目指します．\n",
    "\n",
    "|項目|設計空間|補足|\n",
    "|:--|:--|:--|\n",
    "|学習率（$r$）| $0 < r < 1$| 大きいほど収束が速いが，大きすぎるとうまく学習しない．|\n",
    "|隠れ層のノード数（$n$）| $\\{1, 2,...,6\\}$ |最大でも，入力層のノード数程度？|\n",
    "|最適化アルゴリズム| {Gradient descent, Adam}| 前者はオーソドックス，Adamは新しめ．|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 学習データ，テストデータの生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def make_x(anlzr, title='ONE PIECE'):\n",
    "    worsts = np.array(anlzr.extract_item(title)[1:7])\n",
    "    bests = np.array(anlzr.extract_item(title, 'best')[1:7])\n",
    "    worsts_normalized = worsts / (worsts + bests - 1)    \n",
    "    return worsts_normalized\n",
    "\n",
    "\n",
    "def make_y(anlzr, title='ONE PIECE', thresh_week=10):\n",
    "    return float(len(anlzr.extract_item(title)) <=  thresh_week)\n",
    "\n",
    "\n",
    "def batch_x_y(anlzr, titles, thresh_week=10):\n",
    "    xs = np.array([make_x(anlzr, title) for title in titles])\n",
    "    ys = np.array([[make_y(anlzr, title, thresh_week)] for title in titles])\n",
    "    return xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test_10, y_test_10 = batch_x_y(wj, wj.end_titles[-100:])\n",
    "x_train_10, y_train_10 = batch_x_y(wj, wj.end_titles[:-100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多層パーセプトロンの構築\n",
    "\n",
    "　`ComicNet()`は，多層パーセプトロンを管理するクラスです．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class ComicNet():\n",
    "    def __init__(self, x_train, y_train, x_test, y_test,\n",
    "                 r=0.01, n_x=6, n_h=4, epoch=1000, algo='SGD', log=False):\n",
    "        self.x_train = x_train[:, :n_x]\n",
    "        self.y_train = y_train\n",
    "        self.x_test = x_test[:, :n_x]\n",
    "        self.y_test = y_test\n",
    "        self.learning_rate = r\n",
    "        self.n_x = n_x\n",
    "        self.n_h = n_h\n",
    "        self.epoch = epoch\n",
    "        self.algo = algo\n",
    "        self.log = log\n",
    "                \n",
    "    def build_network(self):\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        n_y = self.y_test.shape[1]\n",
    "        self.x = tf.placeholder(tf.float32, [None, self.n_x], name='x')\n",
    "        self.y = tf.placeholder(tf.float32, [None, n_y], name='y')\n",
    "        \n",
    "        self.w_h = tf.Variable(\n",
    "            tf.truncated_normal((self.n_x, self.n_h), stddev=0.05))\n",
    "        self.b_h = tf.Variable(tf.zeros(self.n_h))\n",
    "        self.logits = tf.add(tf.matmul(self.x, self.w_h), self.b_h)\n",
    "        self.logits = tf.nn.sigmoid(self.logits)\n",
    "        tf.summary.histogram('w_h', self.w_h)\n",
    "        tf.summary.histogram('b_h', self.b_h)\n",
    "        \n",
    "        self.w_y = tf.Variable(\n",
    "            tf.truncated_normal((self.n_h, n_y), stddev=0.05))\n",
    "        self.b_y = tf.Variable(tf.zeros(n_y))\n",
    "        self.logits = tf.add(tf.matmul(self.logits, self.w_y), self.b_y)\n",
    "        self.logits = tf.nn.sigmoid(self.logits, name='logits')\n",
    "        tf.summary.histogram('w_b', self.w_y)\n",
    "        tf.summary.histogram('b_b', self.b_y)\n",
    "        tf.summary.histogram('logits', self.logits)\n",
    "        \n",
    "        self.loss = tf.nn.l2_loss(self.y - self.logits, name='loss')\n",
    "        tf.summary.scalar('loss', self.loss)\n",
    "        \n",
    "        if self.algo == 'SGD':\n",
    "            self.optimizer = tf.train.GradientDescentOptimizer(\n",
    "                self.learning_rate).minimize(self.loss)\n",
    "        else:\n",
    "            self.optimizer = tf.train.AdamOptimizer(\n",
    "                self.learning_rate).minimize(self.loss)\n",
    "        \n",
    "        correct_prediction = tf.equal(self.y, tf.round(self.logits))\n",
    "        self.acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32),\n",
    "            name='acc')\n",
    "        tf.summary.scalar('acc', self.acc)\n",
    "        \n",
    "        self.merged = tf.summary.merge_all()\n",
    "    \n",
    "    \n",
    "    def batch_train_and_valid(self, n_tra, n_val):\n",
    "        i_pos = [i for i in range(len(self.y_train))\n",
    "                 if self.y_train[i]==1]\n",
    "        i_neg = [i for i in range(len(self.y_train))\n",
    "                 if self.y_train[i]==0]\n",
    "            \n",
    "        i_pos_tra = np.random.choice(i_pos, n_tra, replace=False)\n",
    "        i_neg_tra = np.random.choice(i_neg, n_tra, replace=False)\n",
    "        i_pos_val = np.random.choice(\n",
    "            [i for i in i_pos if not i in i_pos_tra], n_val,\n",
    "            replace=False)\n",
    "        i_neg_val = np.random.choice(\n",
    "            [i for i in i_neg if not i in i_neg_tra], n_val,\n",
    "            replace=False)\n",
    "        \n",
    "        x_tra = self.x_train[np.r_[i_pos_tra, i_neg_tra]]\n",
    "        y_tra = self.y_train[np.r_[i_pos_tra, i_neg_tra]]\n",
    "        x_val = self.x_train[np.r_[i_pos_val, i_neg_val]]\n",
    "        y_val = self.y_train[np.r_[i_pos_val, i_neg_val]]\n",
    "        \n",
    "        return x_tra, y_tra, x_val, y_val\n",
    "        \n",
    "        \n",
    "    def train(self):\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            log_tra = 'logs/n_x={}/tra/r={},n_h={},algo={}'\\\n",
    "                .format(self.n_x, self.learning_rate, self.n_h, self.algo)\n",
    "            writer_tra = tf.summary.FileWriter(log_tra)\n",
    "            log_val = 'logs/n_x={}/val/r={},n_h={},algo={}'\\\n",
    "                .format(self.n_x, self.learning_rate, self.n_h, self.algo)\n",
    "            writer_val = tf.summary.FileWriter(log_val)\n",
    "\n",
    "            for e in range(self.epoch):\n",
    "                n_tra = 50\n",
    "                n_val = (self.y_train == 1).sum() - n_tra\n",
    "                x_tra, y_tra, x_val, y_val = self.batch_train_and_valid(n_tra, n_val)\n",
    "                \n",
    "                feed_dict = {self.x: x_tra, self.y: y_tra}\n",
    "                _, loss_tra, acc_tra, mer_tra = sess.run(\n",
    "                        (self.optimizer, self.loss, self.acc, self.merged), \n",
    "                        feed_dict=feed_dict)\n",
    "                \n",
    "                feed_dict = {self.x: x_val, self.y: y_val}\n",
    "                loss_val, acc_val, mer_val = sess.run(\n",
    "                    (self.loss, self.acc, self.merged),\n",
    "                    feed_dict=feed_dict)\n",
    "                \n",
    "                if self.log:\n",
    "                    writer_tra.add_summary(mer_tra, e)\n",
    "                    writer_val.add_summary(mer_val, e)\n",
    "            \n",
    "            self.save_model_path = './prediction_model'\n",
    "            saver = tf.train.Saver()\n",
    "            save_path = saver.save(sess, self.save_model_path)\n",
    "            \n",
    "            \n",
    "    def test(self):\n",
    "        loaded_graph = tf.Graph()\n",
    "        \n",
    "        with tf.Session(graph=loaded_graph) as sess:\n",
    "            loader = tf.train.import_meta_graph(\n",
    "                self.save_model_path + '.meta')\n",
    "            loader.restore(sess, self.save_model_path)\n",
    "            \n",
    "            x_loaded = loaded_graph.get_tensor_by_name('x:0')\n",
    "            y_loaded = loaded_graph.get_tensor_by_name('y:0')\n",
    "            \n",
    "            loss_loaded = loaded_graph.get_tensor_by_name('loss:0')\n",
    "            acc_loaded = loaded_graph.get_tensor_by_name('acc:0')\n",
    "            logits_loaded = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        \n",
    "            feed_dict = {x_loaded: self.x_test, y_loaded: self.y_test}\n",
    "            loss_test, acc_test, logits_test = sess.run(\n",
    "                (loss_loaded, acc_loaded, logits_loaded), feed_dict=feed_dict)\n",
    "            print('acc_test:', acc_test)\n",
    "            return logits_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ハイパーパラメータの調整\n",
    "\n",
    "　以下では，[TensorBoard](https://www.tensorflow.org/get_started/summaries_and_tensorboard)を使ってハイパーパラメータを調整します．本記事におけるハイパーパラメータは，学習率$r$，隠れ層のノード数$n$，そして最適化アルゴリズム（SGD，Adam）です．まずは，アタリをつけるため，ざっくりとパラメータを振って検証データに対する精度を比較します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_x=1, r=0.1, n_h=1, algo=SGD\n",
      "n_x=1, r=0.1, n_h=1, algo=Adam\n",
      "n_x=1, r=0.05, n_h=1, algo=SGD\n"
     ]
    }
   ],
   "source": [
    "for n_x in range(1, 7):\n",
    "    for r in [0.1, 0.05, 0.001, 0.005]:\n",
    "        for n_h in range(1, n_x + 1):\n",
    "            for algo in ['SGD', 'Adam']:\n",
    "                print('n_x={}, r={}, n_h={}, algo={}'.\n",
    "                      format(str(n_x), str(r), str(n_h), algo))\n",
    "                wjnet = ComicNet(x_train_10, y_train_10, x_test_10, y_test_10, \n",
    "                                 n_x=n_x, r=r, n_h=n_h, algo=algo, epoch=10000, log=True)\n",
    "                wjnet.build_network()\n",
    "                wjnet.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 追加実験1. 20週以内に終了する作品の識別\n",
    "\n",
    "　多くの方にご指摘された通り，10週以内に終了する作品は凄く稀です．以下は，歴代ジャンプ作品の掲載期間のヒストグラムです．\n",
    " \n",
    "　なお，`thresh_week`を調整すれば，任意の期間内に終了する作品の識別問題に挑戦することができます．ご興味のある方はぜひ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 追加実験2. 連載期間の回帰\n",
    "\n",
    "　短命作品か否かの識別問題だけではなく，連載期間の回帰問題にも挑戦します．多層パーセプトロンは以下です．短命作品の識別問題との違いは，出力層の活性化関数が無くなっていることです．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
